{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook trains a convolutional neural network (CNN) for image classification on a typical dataset (CIFAR-100 for now). The dataset consists of 100 labels with 600 images per label. 500 images/label are in the train set and 100 images/label are in the test set.\n",
    "\n",
    "Training can be offloaded to a GPU by choosing the appropriate value for the variable 'device' below.\n",
    "\n",
    "Since the goal is to compare run-times and pricing, we aim for consistency (fixed model architecture, dataset, hyperparameters etc.) rather than tricks to speed up learning or improve model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.models import resnet34\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(device=='cuda') #don't run this notebook if GPU not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are not using a pretrained network, we do **not** need to use these normalization constants \n",
    "# but we will do so anyway in case we want to compare the performance to a pre-trained network.\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_data():\n",
    "    '''Download dataset and apply preprocessing transforms.\n",
    "    '''\n",
    "    LOCAL_PATH = './data/CIFAR100'\n",
    "\n",
    "    train_data = CIFAR100(LOCAL_PATH, \n",
    "                          train=True, \n",
    "                          download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Resize((224, 224)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=MEAN, std=STD)\n",
    "                          ])\n",
    "                         )\n",
    "\n",
    "    test_data = CIFAR100(LOCAL_PATH,\n",
    "                         train=False, \n",
    "                         download=True,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.Resize((224, 224)),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=MEAN, std=STD)\n",
    "                         ])\n",
    "                        )\n",
    "\n",
    "    label_names = pickle.load(open(f'{LOCAL_PATH}/cifar-100-python/meta', \"rb\"), encoding='ISO-8859-1')[\"fine_label_names\"]\n",
    "\n",
    "    return train_data, test_data, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, label_names = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_data, test_data, batch_size, pin_memory=True):\n",
    "    '''Create dataloaders that create batches for training.\n",
    "    '''\n",
    "    train_dl = DataLoader(train_data, \n",
    "                          batch_size=batch_size, \n",
    "                          pin_memory=pin_memory)\n",
    "\n",
    "    test_dl = DataLoader(test_data, \n",
    "                         batch_size=batch_size, \n",
    "                         pin_memory=pin_memory)\n",
    "\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = create_dataloaders(train_data, test_data, 128, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Loss Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(pretrained=False)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "\n",
    "    model.train()  # switch to train model (for dropout, batch normalization etc.)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    acc_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs):  # loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "\n",
    "        for idx, (images, labels) in enumerate(train_dl):  # loop over batches\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            curr_loss += loss.item()  # accumulate loss\n",
    "            N += len(labels)  # accumulate number of images seen in this epoch\n",
    "\n",
    "            # backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % print_freq == 0 or epoch == N_epochs-1:\n",
    "            val_loss, val_acc = validate(test_dl, model, criterion)  # get model perf metrics from test set\n",
    "\n",
    "            acc_dict[epoch] = val_acc\n",
    "            loss_dict[epoch] = val_loss\n",
    "\n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_acc = {val_acc}')\n",
    "\n",
    "    return model, acc_dict, loss_dict\n",
    "\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval()  # switch to eval model\n",
    "\n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    N_correct = 0\n",
    "\n",
    "    with torch.no_grad():  # no need to keep variables for backprop computations\n",
    "        for idx, (images, labels) in enumerate(test_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            preds_nonprob = preds.argmax(dim=1)\n",
    "\n",
    "            N_correct += (labels == preds_nonprob).sum().item()  # accuracy computation\n",
    "\n",
    "            loss += criterion(preds, labels)  # cumulative loss\n",
    "            N += len(labels)\n",
    "\n",
    "    return loss / N, N_correct/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time model, acc_dict, loss_dict = train_model(train_dl, test_dl, model, criterion, 20, 1)\n",
    "%time model, acc_dict, loss_dict = train_model(train_dl, test_dl, model, criterion, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
